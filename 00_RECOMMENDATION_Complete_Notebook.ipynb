{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for mongo\n",
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "import pandas as pd\n",
    "\n",
    "#for cleaning\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "#for modelling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#for recommender\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4 Complete Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Upload Book Data to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup config dictionary to connect to Google Cloud Compute, Mongo\n",
    "\n",
    "config = {\n",
    "  'host': '34.94.12.159:27017',\n",
    "  'username': 'mongo_user',\n",
    "  'password': 'mongo',\n",
    "  'authSource': 'book'\n",
    "}\n",
    "\n",
    "#connect to MongoClient\n",
    "client = MongoClient(**config)\n",
    "\n",
    "#assign database a variable name\n",
    "db = client.book\n",
    "\n",
    "#assign collection to database\n",
    "review_col = db.review\n",
    "interaction_col = db.interaction\n",
    "book_col = db.book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format JSON dictionaries into a list\n",
    "data = [json.loads(line) for line in open('goodreads_books_children.json', 'r')]\n",
    "data_review = [json.loads(line) for line in open('goodreads_reviews_children.json', 'r')]\n",
    "data_interaction = [json.loads(line) for line in open('goodreads_interactions_children.json', 'r')]\n",
    "\n",
    "#Insert lists into mongo\n",
    "book_col.insert_many(data)\n",
    "review_col.insert_many(data_review)\n",
    "interaction_col.insert_many(data_interaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Pull Data from MongoDB with pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create List of 500 Most Reviewed 5 Star Illustrated books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Create List of Most Positively Reviewed books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get book_id's for top rated, highest volume rated books\n",
    "pipeline = [\n",
    "    {'$match': {'rating': 5}},                                     # filter to 5 star books\n",
    "    {'$group': {'_id': '$book_id', 'num_reviews': {'$sum': 1}}},   # group reviews by book_id and sum \n",
    "    {'$project': {'_id': 0, 'book_id': '$_id', 'num_reviews': 1}}, # get book_id and num_reviews\n",
    "    {'$sort': {'num_reviews': -1}}                                 # sort by num of reviews, descending\n",
    "]\n",
    "\n",
    "#put book_ids into a list with highly reviewed books on top\n",
    "num_reviews_list_desc = list(review_col.aggregate(pipeline))\n",
    "\n",
    "#put book ids list into a DF\n",
    "num_review_df = pd.DataFrame(num_reviews_list_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Create List of Illustrated books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query number of illustrated books\n",
    "query = { 'description': { \"$regex\": \"illustration*\" } }\n",
    "docs = book_col.count_documents( query )\n",
    "docs\n",
    "#12182\n",
    "\n",
    "#insert illustrated books ids into a DF\n",
    "illustrated_df = pd.DataFrame(list(book_col.find(query, {\"_id\":0,\"book_id\":1, 'title':1})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Create ranked illustrated book list based on positive review volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inner Merge Illustrated Books DF with Max Num Reviews DF to find most popular illustrated books\n",
    "illustrated_by_num_review = num_review_df.merge(illustrated_df, how = \"inner\", on = \"book_id\")\n",
    "\n",
    "#save ranked df to csv\n",
    "illustrated_by_num_review.to_csv(\"illustrated_df_by_num_review.csv\")\n",
    "\n",
    "#put top 500 book_id's in a list\n",
    "top_500_book_id = []\n",
    "for i in range(500):\n",
    "    top_500_book_id.append(illustrated_by_num_review.iloc[i][\"book_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Pull Review and Book Data of 500 Most Reviewed 5 Star Illustrated books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Book Data of 500 Most Reviewed 5 Star Illustrated books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_500_book_id = [str(x) for x in top_500_book_id] #change numbers to string type\n",
    "\n",
    "#aggregate book dfs\n",
    "book_agg = []\n",
    "\n",
    "#make dataframe per book\n",
    "for book in top_500_book_id:\n",
    "    data = pd.DataFrame(list(book_col.find({'book_id': book}, {\"_id\":0, \"book_id\":1, 'description':1, 'title':1, 'publication_year':1, 'image_url':1, 'isbn13':1, 'num_pages':1, 'publisher':1})))\n",
    "    book_agg.append(data)\n",
    "\n",
    "#make master dataframe\n",
    "top_500_book_info_df = pd.concat(book_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Review Data of 500 Most Reviewed 5 Star Illustrated books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregate review dfs\n",
    "df_agg = []\n",
    "\n",
    "#make dataframe per book\n",
    "for book in top_500_book_id:\n",
    "    data = pd.DataFrame(list(review_col.find({'book_id': book, 'rating':5}, {\"_id\":0, \"book_id\":1, 'rating':1, 'review_text':1}).limit(100)))\n",
    "    data['text'] = data[[\"book_id\",'review_text']].groupby([\"book_id\"])['review_text'].transform(lambda x: ','.join(x))\n",
    "    data = data[[\"book_id\",'text']].drop_duplicates()\n",
    "    df_agg.append(data)\n",
    "\n",
    "#concat review dfs\n",
    "top_500_review_df = pd.concat(df_agg)\n",
    "top_500_review_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate Book + Review Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_review_df = pd.merge(review_df_str, top_500_book_info_df,  on=['book_id'])\n",
    "book_review_df.to_csv(\"book_review_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)\n",
    "\n",
    "# Apply a second round of cleaning\n",
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    #text = re.sub(r'\\b\\w{,2}\\b', '', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Cleaning to Titles, Review Text, Book Description Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a cleaned title column to remove duplicates\n",
    "book_review_df[\"cleaned_title\"] = book_review_df.title\n",
    "book_review_df[\"cleaned_title\"] = book_review_df.cleaned_title.apply(round1)\n",
    "\n",
    "#drop duplicate titles\n",
    "book_review_df_nodup = book_review_df.drop_duplicates(subset=\"cleaned_title\", keep = 'first')\n",
    "book_review_df_nodup.to_csv(\"book_review_df_nodup.csv\")\n",
    "\n",
    "#isolate columns we want\n",
    "#isolate review df, pickle to model with that\n",
    "review_df = pd.DataFrame(book_review_df[[\"text\"]])\n",
    "\n",
    "#clean review_df\n",
    "review_df[\"text\"] = review_df[\"text\"].apply(round1)\n",
    "review_df[\"text\"] = review_df[\"text\"].apply(round2)\n",
    "review_df.to_csv(\"review_df_nodup.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis + Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Book Review Impression on Quality of Illustration vs Quality of story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', ngram_range = (1,2), max_df = .95, min_df = .1)\n",
    "data_cv = cv.fit_transform(review_df.text)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = review_df.index\n",
    "data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See what most common words are\n",
    "data_dtm_copy = data_dtm\n",
    "word_df = pd.DataFrame(data_dtm.sum())\n",
    "word_df = word_df.sort_values(by = 0, ascending = False)\n",
    "word_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isolate words about plot, words about art\n",
    "data_dtm_copy[\"words_about_plot\"] = data_dtm[\"story\"] + data_dtm[\"tale\"] + data_dtm[\"tales\"] + data_dtm[\"plot\"] + \n",
    "                                    data_dtm[\"stories\"] + data_dtm[\"written\"] + data_dtm[\"writing\"] + data_dtm[\"writer\"] + \n",
    "                                    data_dtm[\"text\"] +  data_dtm[\"reader\"]\n",
    "\n",
    "data_dtm_copy[\"words_about_art\"] = data_dtm[\"illustrations\"] + data_dtm[\"illustration\"] + data_dtm[\"artist\"] + \n",
    "                                    data_dtm[\"picture\"] + data_dtm[\"illustrator\"] + data_dtm[\"beautiful\"]\n",
    "\n",
    "total_words_about_art_plot = data_dtm_copy[\"words_about_plot\"] + data_dtm_copy[\"words_about_art\"]\n",
    "\n",
    "data_dtm_copy[\"art_over_plot\"] = data_dtm_copy[\"words_about_art\"]/total_words_about_art_plot\n",
    "                                \n",
    "data_dtm_copy[\"plot_over_art\"] = data_dtm_copy[\"words_about_plot\"]/total_words_about_art_plot\n",
    "\n",
    "illustration_vs_story_df = data_dtm_copy[[\"art_over_plot\", \"plot_over_art\"]]\n",
    "illustration_vs_story_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illustration_vs_story_df.to_csv(\"illustration_vs_story_df.csv\")\n",
    "# Let's pickle it for later use\n",
    "data_dtm.to_pickle(\"dtm_review_nodup.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Topics from Book Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        \n",
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)\n",
    "\n",
    "def adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the  adjectives.'''\n",
    "    is_adj = lambda pos: pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    adj = [word for (word, pos) in pos_tag(tokenized) if is_adj(pos)] \n",
    "    return ' '.join(adj)\n",
    "\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"book_clean_df.pkl\",\"rb\")\n",
    "data_clean  = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF on nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noun filter on df\n",
    "data_nouns = pd.DataFrame(data_clean.description.apply(nouns))\n",
    "data_nouns\n",
    "\n",
    "# adjective filter on df\n",
    "data_adj = pd.DataFrame(data_clean.description.apply(adj))\n",
    "data_adj\n",
    "\n",
    "# noun adj filter on df\n",
    "data_nouns_adj = pd.DataFrame(data_clean.description.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "stop_noun = [\"peter\", \"pages\", \"mrs\", \"beatrix\", \"potter\", \"also\", \"national\", \"appeal\", \"everyone\", \"literature\", \n",
    "             \"nothing\", \"detailed\", \"everywhere\", \"everything\", \"detailed\", \"publishers weekly\", \"adults\", \n",
    "             \"ever\", \"finally\", \"parent\", \"need\", \"also\", \"needs\", \"fans\", \"asks\", \"captures\", \"gift\", \"five\", \n",
    "             \"detail\", \"others\",  \"details\", \"brought life\", \"caldecott\", \"readers\", \"tale\", \"tales\", \"young\", \n",
    "             \"years\", \"ages\", \"seuss\", \"series\", \"color\", \"day\", \"medal\", \"collier\", \"review\", \"seriers\", \"award\", \n",
    "             \"thing\", \"stories\", \"child\", \"life\", \"things\", \"childhood\", \"year\", \"world\", \"award\", \"winner\", \"york\", \n",
    "             \"caldecott\", \"times\", \"new\", \"one\", \"author\", \"edition\", \"readers\", \"reader\", \"illustrator\", \"word\", \n",
    "             \"words\", \"little\", \"text\", \"illustration\", \"illustrations\", \"story\", \"picture\", \"best\", \"pictures\", \n",
    "             \"children\", 'love', 'great', 'book', 'books', 'read', 'reading', 'just', 'like', 'children', 'loved', \n",
    "             'time', 'kids', 'fun', 'really', 'reading', 'way', 'favorite', 'page', 'wonderful']\n",
    "\n",
    "stop_words_noun_agg = text.ENGLISH_STOP_WORDS.union(stop_noun)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "tv_noun = TfidfVectorizer(stop_words=stop_words_noun_agg, ngram_range = (1,2), max_df = .6, min_df = .01)\n",
    "data_tv_noun = tv_noun.fit_transform(data_nouns.description)\n",
    "data_dtm_noun = pd.DataFrame(data_tv_noun.toarray(), columns=tv_noun.get_feature_names())\n",
    "data_dtm_noun.index = data_nouns.index\n",
    "data_dtm_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(4)\n",
    "doc_topic = nmf_model.fit_transform(data_tv_noun)\n",
    "\n",
    "display_topics(nmf_model, tv_noun.get_feature_names(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model_noun = NMF(20)\n",
    "doc_topic_noun = nmf_model_noun.fit_transform(data_tv_noun)\n",
    "\n",
    "display_topics(nmf_model_noun, tv_noun.get_feature_names(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_noun = pd.DataFrame(doc_topic_noun.round(5),\n",
    "             index = data_clean.index,\n",
    "             columns = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19])\n",
    "\n",
    "H_noun.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize NMF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_noun[\"sum\"] = H_noun.sum(axis=1)\n",
    "\n",
    "for num in range(0,20):\n",
    "    H_noun[num] = H_noun[num]/H_noun[\"sum\"]\n",
    "\n",
    "H_noun = H_noun.drop(columns = \"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_noun.columns = [\"BOY\", \"FARM ANIMAL\", \"CAT\", \"SCHOOL\", \"GIRL\", \n",
    "                        \"RABBIT\", \"POOH\", \"BABY\", \"BEAR\", \"ART\", \"FAMILY\",  \n",
    "                        \"DOG\", \"DRAGONS\", \"MICE\", \"ADVENTURE\", \"UNKNOWN?\", \n",
    "                        \"BEDTIME\", \"HISTORY\", \"ALPHABET\", \"EDUCATIONAL\"]\n",
    "\n",
    "H_noun.to_csv(\"h_noun_percentages.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF on adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "\n",
    "stop_adj = [\"perfect\", \"best\", \"new\", \"good\", \"young\", \"old\", \"little\", \"beautiful\", 'love', 'great', \n",
    "            \"delightful\", 'illustrated', 'read', 'reading', 'just', 'like', 'bear', 'loved', 'tale', 'big', \n",
    "            'fun', 'really', 'reading', 'way', 'favorite', 'page', 'wonderful', \"book book\", \"middle\", \"american\"]\n",
    "\n",
    "stop_words_adj_agg = text.ENGLISH_STOP_WORDS.union(stop_adj)\n",
    "\n",
    "\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "tv_adj = TfidfVectorizer(stop_words=stop_words_adj_agg, ngram_range = (1,2), max_df = .6, min_df = .02)\n",
    "data_tv_adj = tv_adj.fit_transform(data_adj.description)\n",
    "data_dtm_adj = pd.DataFrame(data_tv_adj.toarray(), columns=tv_adj.get_feature_names())\n",
    "data_dtm_adj.index = data_adj.index\n",
    "data_dtm_adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model_adj = NMF(6)\n",
    "doc_topic_adj = nmf_model_adj.fit_transform(data_tv_adj)\n",
    "\n",
    "display_topics(nmf_model_adj, tv_adj.get_feature_names(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_adj = pd.DataFrame(doc_topic_adj.round(5),\n",
    "             index = data_clean.index,\n",
    "             columns = [\"CLASSIC\", \"BLACK AND WHITE\", \"WHIMSICAL\", \"SIMPLE\", \"FUNNY\", \"DIFFERENT\"])\n",
    "H_adj.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_adj[\"sum\"] = H_adj.sum(axis=1)\n",
    "\n",
    "H_adj[\"CLASSIC\"] = H_adj[\"CLASSIC\"]/H_adj[\"sum\"]\n",
    "H_adj[\"BLACK AND WHITE\"] = H_adj[\"BLACK AND WHITE\"]/H_adj[\"sum\"]\n",
    "H_adj[\"WHIMSICAL\"] = H_adj[\"WHIMSICAL\"]/H_adj[\"sum\"]\n",
    "H_adj[\"SIMPLE\"] = H_adj[\"SIMPLE\"]/H_adj[\"sum\"]\n",
    "H_adj[\"FUNNY\"] = H_adj[\"FUNNY\"]/H_adj[\"sum\"]\n",
    "H_adj[\"DIFFERENT\"] = H_adj[\"DIFFERENT\"]/H_adj[\"sum\"]\n",
    "\n",
    "H_adj = H_adj.drop(columns = \"sum\")\n",
    "\n",
    "H_adj.to_csv(\"H_adj_percentiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_noun_adj = pd.merge(H_noun, H_adj, right_index = True, left_index = True)\n",
    "H_noun_adj.to_csv(\"H_noun_adj.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate Features DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.read_csv(\"H_noun_adj_percent.csv\")\n",
    "topics_df = topics_df.fillna(0)\n",
    "\n",
    "rawdata_df = pd.read_csv(\"book_review_df_nodup.csv\")\n",
    "rawdata_df = rawdata_df.fillna(0)\n",
    "\n",
    "illustration_vs_story_df = pd.read_csv(\"illustration_vs_story_df.csv\")\n",
    "illustration_vs_story_df = illustration_vs_story_df.drop(columns = [\"Unnamed: 0\"])\n",
    "\n",
    "topics_df = topics_df.drop(columns = [\"title\"])\n",
    "\n",
    "massive_df = pd.concat([topics_df, rawdata_df], axis=1, join='inner')\n",
    "complete_df = pd.concat([illustration_vs_story_df, massive_df], axis=1, join='inner')\n",
    "\n",
    "complete_df.to_csv(\"complete_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_calc_df = pd.DataFrame(complete_df[['art_over_plot', 'plot_over_art', 'EDUCATIONAL', 'BOY', 'GIRL', \n",
    "                                               'DIFFERENT', 'CLASSIC', 'num_pages', 'WHIMSICAL', 'SIMPLE', 'FUNNY', \n",
    "                                               'ADVENTURE', 'HISTORY', 'ART', 'FAMILY', 'SCHOOL', 'BEDTIME', \n",
    "                                               'DRAGONS', 'FARM ANIMAL', 'BEAR', 'POOH', 'DOG', 'CAT', 'MICE', 'RABBIT', 'BABY', 'ALPHABET',  'BLACK AND WHITE', \n",
    "                                               'title']].set_index('title'))\n",
    "\n",
    "similarity_calc_df[\"BEAR\"] = similarity_calc_df[\"BEAR\"] + similarity_calc_df[\"POOH\"]\n",
    "similarity_calc_df = similarity_calc_df.drop(columns = \"POOH\")\n",
    "\n",
    "similarity_calc_df.to_csv(\"similarity_calc_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmscaler = MinMaxScaler()\n",
    "similarity_calc_df[\"LENGTH_SCALED\"] = mmscaler.fit_transform(similarity_calc_df[[\"LENGTH\"]])\n",
    "similarity_calc_df[\"LENGTH\"] = similarity_calc_df[\"LENGTH_SCALED\"] \n",
    "similarity_calc_df = similarity_calc_df.drop(columns= [\"LENGTH_SCALED\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Recommendation Test Cases : Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_storm = [0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,1,0,1,0,0,1,0]\n",
    "test_mom = [1,0,0,0,1,1,0,200,1,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "test_dad = [1,1,1,1,0,1,0,30,0,0,1,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "test_dict = {\"test_storm\": test_storm, \"test_mom\": test_mom, \"test_dad\" : test_dad}\n",
    "test_dict_df = pd.DataFrame.from_dict(test_dict)\n",
    "test_dict_df = test_dict_df.transpose()\n",
    "test_dict_df \n",
    "\n",
    "test_dict_df[27] = mmscaler.transform(test_dict_df[[7]])\n",
    "test_dict_df[7] = test_dict_df[27]\n",
    "test_dict_df = test_dict_df.drop(columns = [27])\n",
    "test_dict_df.to_csv(\"test_df.csv\") \n",
    "\n",
    "test_dict_df.columns = similarity_calc_df.columns\n",
    "frames = [test_dict_df, similarity_calc_df]\n",
    "full_df = pd.concat(frames)\n",
    "full_df\n",
    "\n",
    "full_df = full_df.fillna(full_df.mean())\n",
    "\n",
    "list_of_titles = similarity_calc_df.index.values.tolist()\n",
    "list_of_titles_full = full_df.index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_df = pd.DataFrame(cosine_similarity(full_df, full_df).round(5), index=list_of_titles_full, columns=list_of_titles_full)\n",
    "cos_sim_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_storm = pd.DataFrame(cos_sim_df.iloc[[0]])\n",
    "df_mom = pd.DataFrame(cos_sim_df.iloc[[1]])\n",
    "df_dad = pd.DataFrame(cos_sim_df.iloc[[2]])\n",
    "\n",
    "df_storm = df_storm.transpose()\n",
    "df_mom = df_mom.transpose()\n",
    "df_dad = df_dad.transpose()\n",
    "\n",
    "df_storm = df_storm.sort_values(\"test_storm\", ascending = False)\n",
    "df_mom  = df_mom.sort_values(\"test_mom\", ascending = False)\n",
    "df_dad = df_dad.sort_values(\"test_dad\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_storm.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mom.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dad.head(7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
